{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'Training_Essay_Data.csv' is the correct path\n",
    "pd.set_option('display.max_rows', 5000000)\n",
    "\n",
    "\n",
    "path = \"final_test.csv\"\n",
    "path = \"megaset.csv\"\n",
    "path = 'train_v2_drct_02.csv'\n",
    "path = 'metadataset.csv'\n",
    "path = 'turboset.csv'\n",
    "path = 'megaset4.csv'\n",
    "path = 'train_essays.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.00001, random_state=42)\n",
    "\n",
    "\n",
    "text_column = 'text'\n",
    "generated_column = 'generated'\n",
    "\n",
    "# Define the clean_word function\n",
    "def clean_word(word):\n",
    "    word = word.lower()  # Lowercase the word\n",
    "    cleaned_word = re.sub(r'(?<!\\w)[\\'\\\"?!;:,.]|(?<=\\s)[\\'\\\"?!;:,.]|[\\'\\\"?!;:,.](?!\\w)', '', word)\n",
    "    return cleaned_word\n",
    "\n",
    "# Tokenize and clean words in essays\n",
    "def tokenize_and_clean(text):\n",
    "    return [clean_word(word) for word in text.lower().split()]\n",
    "\n",
    "# Ensure essays are strings and not empty or NaN\n",
    "train_data.dropna(subset=[text_column], inplace=True)\n",
    "train_data[text_column] = train_data[text_column].astype(str)\n",
    "\n",
    "# Process AI Essays\n",
    "ai_essays = train_data[train_data[generated_column] == 1][text_column]\n",
    "ai_words = []\n",
    "for essay in ai_essays:\n",
    "    ai_words.extend(tokenize_and_clean(essay))\n",
    "ai_word_counts = Counter(ai_words)\n",
    "ai_total_words = sum(ai_word_counts.values())\n",
    "ai_word_freq = {word: count / ai_total_words for word, count in ai_word_counts.items()}\n",
    "\n",
    "# Process Human Essays\n",
    "human_essays = train_data[train_data[generated_column] == 0][text_column]\n",
    "human_words = []\n",
    "for essay in human_essays:\n",
    "    human_words.extend(tokenize_and_clean(essay))\n",
    "human_word_counts = Counter(human_words)\n",
    "human_total_words = sum(human_word_counts.values())\n",
    "human_word_freq = {word: count / human_total_words for word, count in human_word_counts.items()}\n",
    "\n",
    "# Prepare the combined DataFrame\n",
    "word_frequencies_data = []\n",
    "for word in set(ai_word_freq.keys()).union(set(human_word_freq.keys())):\n",
    "    ai_freq = ai_word_freq.get(word, 0)\n",
    "    human_freq = human_word_freq.get(word, 0)\n",
    "    if human_freq > 0 and ai_freq > 0:  # Ensure word exists in both corpuses\n",
    "        prevalence_factor = ai_freq / human_freq if ai_freq > human_freq else -human_freq / ai_freq\n",
    "        word_frequencies_data.append({\n",
    "            'Word': word,\n",
    "            'AI_Frequency': ai_freq,\n",
    "            'Human_Frequency': human_freq,\n",
    "            'Prevalence_Factor': prevalence_factor\n",
    "        })\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "word_frequencies_df = pd.DataFrame(word_frequencies_data)\n",
    "\n",
    "# Sort by 'Prevalence_Factor'\n",
    "word_frequencies_df.sort_values(by='Prevalence_Factor', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# drop frequencies column\n",
    "word_frequencies_df = word_frequencies_df.drop(columns=['AI_Frequency', 'Human_Frequency'])\n",
    "#write back\n",
    "word_frequencies_df.to_csv('prevalence_table2.csv', index=False)\n",
    "\n",
    "#AI essay count\n",
    "ai_essay_count = len(ai_essays)\n",
    "#Human essay count\n",
    "human_essay_count = len(human_essays)\n",
    "\n",
    "print(\"AI Essay Count: \", ai_essay_count)\n",
    "print(\"Human Essay Count: \", human_essay_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "def get_bigrams(words):\n",
    "    \"\"\"Generate bigrams from a list of words.\"\"\"\n",
    "    return zip(words, words[1:])\n",
    "\n",
    "def calculate_bigram_frequencies(essays):\n",
    "    \"\"\"Calculate bigram frequencies in a collection of essays.\"\"\"\n",
    "    bigrams = []\n",
    "    for essay in essays:\n",
    "        words = clean_text(essay)\n",
    "        bigrams.extend(get_bigrams(words))\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    total_bigrams = sum(bigram_counts.values())\n",
    "    bigram_freq = {bigram: count / total_bigrams for bigram, count in bigram_counts.items()}\n",
    "    return bigram_freq\n",
    "\n",
    "# Load dataset\n",
    "path = 'megaset4.csv'  # Update with your actual path\n",
    "path = 'train_essays.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, _ = train_test_split(data, test_size=0.00001, random_state=42)\n",
    "\n",
    "# Ensure essays are strings and not empty or NaN\n",
    "train_data.dropna(subset=['text'], inplace=True)\n",
    "train_data['text'] = train_data['text'].astype(str)\n",
    "\n",
    "# Process AI and Human Essays\n",
    "\n",
    "ai_bigram_freq = calculate_bigram_frequencies(train_data[train_data['generated'] == 1]['text'])\n",
    "print(\"Finished AI bigram freq\")\n",
    "human_bigram_freq = calculate_bigram_frequencies(train_data[train_data['generated'] == 0]['text'])\n",
    "print(\"Finished Human bigram freq\")\n",
    "\n",
    "# Prepare the combined DataFrame\n",
    "bigram_frequencies_data = []\n",
    "for bigram in set(ai_bigram_freq.keys()).union(set(human_bigram_freq.keys())):\n",
    "    ai_freq = ai_bigram_freq.get(bigram, 0)\n",
    "    human_freq = human_bigram_freq.get(bigram, 0)\n",
    "    prevalence_factor = 0\n",
    "    if human_freq > 0 and ai_freq > 0:\n",
    "        prevalence_factor = ai_freq / human_freq if ai_freq >= human_freq else -human_freq / ai_freq\n",
    "        bigram_frequencies_data.append({\n",
    "            'Bigram': ' '.join(bigram),\n",
    "            'Prevalence_Factor': prevalence_factor\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "bigram_frequencies_df = pd.DataFrame(bigram_frequencies_data)\n",
    "\n",
    "# Sort by 'Prevalence_Factor'\n",
    "bigram_frequencies_df.sort_values(by='Prevalence_Factor', ascending=False, inplace=True)\n",
    "\n",
    "# Write to CSV\n",
    "bigram_frequencies_df.to_csv('two_word_prevalence_table2.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "def get_trigrams(words):\n",
    "    \"\"\"Generate trigrams from a list of words.\"\"\"\n",
    "    return zip(words, words[1:], words[2:])\n",
    "\n",
    "def calculate_trigram_frequencies(essays):\n",
    "    \"\"\"Calculate trigram frequencies in a collection of essays.\"\"\"\n",
    "    trigrams = []\n",
    "    for essay in essays:\n",
    "        words = clean_text(essay)\n",
    "        trigrams.extend(get_trigrams(words))\n",
    "    trigram_counts = Counter(trigrams)\n",
    "    total_trigrams = sum(trigram_counts.values())\n",
    "    trigram_freq = {trigram: count / total_trigrams for trigram, count in trigram_counts.items()}\n",
    "    return trigram_freq\n",
    "\n",
    "# Load dataset\n",
    "path = 'megaset4.csv'  # Update with your actual path\n",
    "path = 'train_essays.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, _ = train_test_split(data, test_size=0.00001, random_state=42)\n",
    "\n",
    "# Ensure essays are strings and not empty or NaN\n",
    "train_data.dropna(subset=['text'], inplace=True)\n",
    "train_data['text'] = train_data['text'].astype(str)\n",
    "\n",
    "# Process AI and Human Essays\n",
    "ai_trigram_freq = calculate_trigram_frequencies(train_data[train_data['generated'] == 1]['text'])\n",
    "print(\"Finished AI bigram freq\")\n",
    "human_trigram_freq = calculate_trigram_frequencies(train_data[train_data['generated'] == 0]['text'])\n",
    "print(\"Finished Human bigram freq\")\n",
    "\n",
    "# Prepare the combined DataFrame\n",
    "trigram_frequencies_data = []\n",
    "for trigram in set(ai_trigram_freq.keys()).union(set(human_trigram_freq.keys())):\n",
    "    ai_freq = ai_trigram_freq.get(trigram, 0)\n",
    "    human_freq = human_trigram_freq.get(trigram, 0)\n",
    "    prevalence_factor = 0\n",
    "    if human_freq > 0 and ai_freq > 0:\n",
    "        prevalence_factor = ai_freq / human_freq if ai_freq >= human_freq else -human_freq / ai_freq\n",
    "        trigram_frequencies_data.append({\n",
    "            'Trigram': ' '.join(trigram),\n",
    "            'Prevalence_Factor': prevalence_factor\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "trigram_frequencies_df = pd.DataFrame(trigram_frequencies_data)\n",
    "\n",
    "# Sort by 'Prevalence_Factor'\n",
    "trigram_frequencies_df.sort_values(by='Prevalence_Factor', ascending=False, inplace=True)\n",
    "\n",
    "# Write to CSV\n",
    "trigram_frequencies_df.to_csv('three_word_prevalence_table2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "def get_fourgrams(words):\n",
    "    \"\"\"Generate fourgrams from a list of words.\"\"\"\n",
    "    return zip(words, words[1:], words[2:], words[3:])\n",
    "\n",
    "def calculate_fourgram_frequencies(essays):\n",
    "    \"\"\"Calculate fourgram frequencies in a collection of essays.\"\"\"\n",
    "    fourgrams = []\n",
    "    for essay in essays:\n",
    "        words = clean_text(essay)\n",
    "        fourgrams.extend(get_fourgrams(words))\n",
    "    fourgram_counts = Counter(fourgrams)\n",
    "    total_fourgrams = sum(fourgram_counts.values())\n",
    "    fourgram_freq = {fourgram: count / total_fourgrams for fourgram, count in fourgram_counts.items()}\n",
    "    return fourgram_freq\n",
    "\n",
    "# Load dataset\n",
    "path = 'megaset4.csv'  # Update with your actual path\n",
    "path = 'train_essays.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, _ = train_test_split(data, test_size=0.00001, random_state=42)\n",
    "\n",
    "# Ensure essays are strings and not empty or NaN\n",
    "train_data.dropna(subset=['text'], inplace=True)\n",
    "train_data['text'] = train_data['text'].astype(str)\n",
    "\n",
    "# Process AI and Human Essays\n",
    "ai_fourgram_freq = calculate_fourgram_frequencies(train_data[train_data['generated'] == 1]['text'])\n",
    "human_fourgram_freq = calculate_fourgram_frequencies(train_data[train_data['generated'] == 0]['text'])\n",
    "\n",
    "# Prepare the combined DataFrame\n",
    "fourgram_frequencies_data = []\n",
    "for fourgram in set(ai_fourgram_freq.keys()).union(set(human_fourgram_freq.keys())):\n",
    "    ai_freq = ai_fourgram_freq.get(fourgram, 0)\n",
    "    human_freq = human_fourgram_freq.get(fourgram, 0)\n",
    "    prevalence_factor = 0\n",
    "    if human_freq > 0 and ai_freq > 0:\n",
    "        prevalence_factor = ai_freq / human_freq if ai_freq >= human_freq else -human_freq / ai_freq\n",
    "        fourgram_frequencies_data.append({\n",
    "            'Fourgram': ' '.join(fourgram),\n",
    "            'Prevalence_Factor': prevalence_factor\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "fourgram_frequencies_df = pd.DataFrame(fourgram_frequencies_data)\n",
    "\n",
    "# Sort by 'Prevalence_Factor'\n",
    "fourgram_frequencies_df.sort_values(by='Prevalence_Factor', ascending=False, inplace=True)\n",
    "\n",
    "# Write to CSV\n",
    "fourgram_frequencies_df.to_csv('four_word_prevalence_table.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "def get_fivegrams(words):\n",
    "    \"\"\"Generate fivegrams from a list of words.\"\"\"\n",
    "    return zip(words, words[1:], words[2:], words[3:], words[4:])\n",
    "\n",
    "def calculate_fivegram_frequencies(essays):\n",
    "    \"\"\"Calculate fivegram frequencies in a collection of essays.\"\"\"\n",
    "    fivegrams = []\n",
    "    for essay in essays:\n",
    "        words = clean_text(essay)\n",
    "        fivegrams.extend(get_fivegrams(words))\n",
    "    fivegram_counts = Counter(fivegrams)\n",
    "    total_fivegrams = sum(fivegram_counts.values())\n",
    "    fivegram_freq = {fivegram: count / total_fivegrams for fivegram, count in fivegram_counts.items()}\n",
    "    return fivegram_freq\n",
    "\n",
    "# Load dataset\n",
    "path = 'megaset4.csv'  # Update with your actual path\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, _ = train_test_split(data, test_size=0.00001, random_state=42)\n",
    "\n",
    "# Ensure essays are strings and not empty or NaN\n",
    "train_data.dropna(subset=['text'], inplace=True)\n",
    "train_data['text'] = train_data['text'].astype(str)\n",
    "\n",
    "# Process AI and Human Essays\n",
    "ai_fivegram_freq = calculate_fivegram_frequencies(train_data[train_data['generated'] == 1]['text'])\n",
    "human_fivegram_freq = calculate_fivegram_frequencies(train_data[train_data['generated'] == 0]['text'])\n",
    "\n",
    "# Prepare the combined DataFrame\n",
    "fivegram_frequencies_data = []\n",
    "for fivegram in set(ai_fivegram_freq.keys()).union(set(human_fivegram_freq.keys())):\n",
    "    ai_freq = ai_fivegram_freq.get(fivegram, 0)\n",
    "    human_freq = human_fivegram_freq.get(fivegram, 0)\n",
    "    prevalence_factor = 0\n",
    "    if human_freq > 0 and ai_freq > 0:\n",
    "        prevalence_factor = ai_freq / human_freq if ai_freq >= human_freq else -human_freq / ai_freq\n",
    "        fivegram_frequencies_data.append({\n",
    "            'Fivegram': ' '.join(fivegram),\n",
    "            'Prevalence_Factor': prevalence_factor\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "fivegram_frequencies_df = pd.DataFrame(fivegram_frequencies_data)\n",
    "\n",
    "# Sort by 'Prevalence_Factor'\n",
    "fivegram_frequencies_df.sort_values(by='Prevalence_Factor', ascending=False, inplace=True)\n",
    "\n",
    "# Write to CSV\n",
    "fivegram_frequencies_df.to_csv('five_word_prevalence_table.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "# Calculate word frequencies in essays\n",
    "def calculate_word_frequencies(essays):\n",
    "    \"\"\"Calculate word frequencies in a collection of essays.\"\"\"\n",
    "    words = []\n",
    "    for essay in essays:\n",
    "        words.extend(clean_text(essay))\n",
    "    word_counts = Counter(words)\n",
    "    total_words = sum(word_counts.values())\n",
    "    word_freq = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return word_freq\n",
    "\n",
    "# Load dataset\n",
    "path = 'megaset4.csv'  # Update with your actual path\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Split data based on readability into 5 groups\n",
    "data['readability_group'] = pd.qcut(data['readability'], 5, labels=range(1, 6))\n",
    "\n",
    "print(data.groupby('readability_group')['readability'].apply(lambda x: (x.min(), x.max())).to_dict())\n",
    "\n",
    "\n",
    "# Loop through each readability group to calculate and save weighted prevalences\n",
    "for group in range(1, 6):\n",
    "    group_data = data[data['readability_group'] == group]\n",
    "    \n",
    "    # Split the group data into AI and Human subsets\n",
    "    ai_data = group_data[group_data['generated'] == 1]['text']\n",
    "    human_data = group_data[group_data['generated'] == 0]['text']\n",
    "    \n",
    "    # Calculate word frequencies for AI and Human essays\n",
    "    ai_word_freq = calculate_word_frequencies(ai_data)\n",
    "    human_word_freq = calculate_word_frequencies(human_data)\n",
    "    \n",
    "    # Prepare the combined DataFrame for prevalence factors\n",
    "    prevalence_data = []\n",
    "    for word in set(ai_word_freq.keys()).union(set(human_word_freq.keys())):\n",
    "        ai_freq = ai_word_freq.get(word, 0)\n",
    "        human_freq = human_word_freq.get(word, 0)\n",
    "        if human_freq > 0 and ai_freq > 0:  # Ensure word exists in both corpuses\n",
    "            prevalence_factor = ai_freq / human_freq if ai_freq > human_freq else -human_freq / ai_freq\n",
    "            prevalence_data.append({\n",
    "            'word': word,\n",
    "            'prevalence': prevalence_factor\n",
    "        })\n",
    "    # Create DataFrame and sort by 'prevalence'\n",
    "    prevalence_df = pd.DataFrame(prevalence_data)\n",
    "    prevalence_df.sort_values(by='prevalence', ascending=False, inplace=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    prevalence_df.to_csv(f'weighted_prevalence{group}.csv', index=False)\n",
    "\n",
    "print(\"Weighted prevalence tables created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import complex\n",
    "\n",
    "prevalence_table = word_frequencies_df\n",
    "prevalence_dict = prevalence_table.set_index('Word')['Prevalence_Factor'].to_dict()\n",
    "\n",
    "# test_data = pd.read_csv('Essays/Training_Essay_Data.csv')\n",
    "# test_data = pd.read_csv('undetectable.csv')\n",
    "test_data = pd.read_csv('megaset.csv').sample(10000)\n",
    "test_data = pd.read_csv('human_expert_essays.csv')\n",
    "test_data = pd.read_csv('human.csv')\n",
    "test_data = pd.read_csv('train_v2_drct_02.csv').sample(10000)\n",
    "test_data = pd.read_csv('metadataset.csv').sample(10000)\n",
    "test_data = pd.read_csv('final_test2.csv').sample(10000)\n",
    "\n",
    "\n",
    "prev = []\n",
    "\n",
    "def scan_text_for_ai(text, print2=False):\n",
    "    # Initialize the prevalence score\n",
    "    prevalence_score = 0\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Calculate Prevalence for each word in the text using the dictionary for fast lookup\n",
    "    for word in words:\n",
    "        word = clean_word(word)\n",
    "        # check if the word is in the prevalence dictionary\n",
    "        if word in prevalence_dict:\n",
    "            prevalence = prevalence_dict.get(word, 0)\n",
    "        else:\n",
    "            prevalence = 0\n",
    "\n",
    "        if (prevalence > 0):\n",
    "            prevalence -= 1\n",
    "        else:\n",
    "            prevalence += 1\n",
    "        prevalence_score += prevalence\n",
    "    prev.append(prevalence_score)\n",
    "    \n",
    "    if print2:\n",
    "        print(prevalence_score)\n",
    "\n",
    "    # verdict2 = predict_ai(text)\n",
    "\n",
    "    # if (verdict2 == 0):\n",
    "    #      return 0\n",
    "    # if (verdict2 == 1):\n",
    "    #     return 1\n",
    "   \n",
    "    # Calculate Verdict based on the total prevalence score\n",
    "    if prevalence_score <= 0:\n",
    "        verdict =  0  #1 / prevalence_score  \n",
    "    else:\n",
    "        verdict =  1 #1 - 1 / prevalence_score\n",
    "    \n",
    "    return verdict\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "# Apply the function to each essay to get a verdict\n",
    "test_data['Verdict'] = test_data['text'].apply(scan_text_for_ai)\n",
    "\n",
    "# Round the 'Verdict' values\n",
    "test_data['Verdict'] = test_data['Verdict'].apply(round)\n",
    "\n",
    "# You can now safely create the 'Prediction' column\n",
    "test_data['Prediction'] = test_data['Verdict']\n",
    "\n",
    "# generated_column = 'label'\n",
    "generated_column = 'generated'\n",
    "# Calculate overall accuracy and error rates\n",
    "accuracy = (test_data['Prediction'] == test_data[generated_column]).mean()\n",
    "fp = test_data[(test_data['Prediction'] == 1) & (test_data[generated_column] == 0)].shape[0]\n",
    "fn = test_data[(test_data['Prediction'] == 0) & (test_data[generated_column] == 1)].shape[0]\n",
    "tp = test_data[(test_data['Prediction'] == 1) & (test_data[generated_column] == 1)].shape[0]\n",
    "tn = test_data[(test_data['Prediction'] == 0) & (test_data[generated_column] == 0)].shape[0]\n",
    "\n",
    "digits = 8\n",
    "accuracy = round(100 * accuracy, digits)\n",
    "\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "fpr = round(100 * fpr, digits)\n",
    "fnr = round(100 * fnr, digits)\n",
    "                                   \n",
    "\n",
    "def mean(listx):\n",
    "    total = 0\n",
    "    for i in range(len(listx)):\n",
    "        total += listx[i]\n",
    "    return total / len(listx)\n",
    "    \n",
    "\n",
    "print(f\"Accuracy: {accuracy}%\")\n",
    "print(f\"False Positive Rate (FPR): {fpr}%\")\n",
    "print(f\"False Negative Rate (FNR): {fnr}%\")\n",
    "print(f\"Average prevalence = {mean(prev)}%\")\n",
    "print(f\"Prevalence std = {np.std(prev)}%\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "verdict = scan_text_for_ai(text, True)\n",
    "print(verdict)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

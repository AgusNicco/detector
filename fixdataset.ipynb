{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import complex\n",
    "\n",
    "# Define the provided functions here\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.lower()  # Lowercase the word\n",
    "    # Remove leading/trailing/inter-word punctuation\n",
    "    cleaned_word = re.sub(r'(?<!\\w)[\\'\\\"?!;:,.]|(?<=\\s)[\\'\\\"?!;:,.]|[\\'\\\"?!;:,.](?!\\w)', '', word)\n",
    "    return cleaned_word\n",
    "\n",
    "def clean_text(text):\n",
    "    words = text.split()\n",
    "    cleaned_words = [clean_word(word) for word in words if clean_word(word)]\n",
    "    return cleaned_words\n",
    "\n",
    "def calculate_complexity(text: str, dic: dict) -> float:\n",
    "    words = clean_text(text)\n",
    "    complexity = 0.0\n",
    "    values = []\n",
    "    for word in words:\n",
    "        if dic.get(word) is not None:\n",
    "            complexity += dic[word]\n",
    "            values.append(dic[word])\n",
    "    if values:\n",
    "        return np.median(values)\n",
    "    else:\n",
    "        return complexity  # Return 0 or a default value if no words match\n",
    "\n",
    "\n",
    "words = complex.read_file_to_dic('vocabulary/top_english_verbs_lower_100000.txt')\n",
    "\n",
    "# Load the datasets\n",
    "df1 = pd.read_csv('Essays/ai_generated_essays_llm_detect_kaggle.csv')\n",
    "\n",
    "\n",
    "# Standardize column names\n",
    "# df1.rename(columns={'label': 'generated'}, inplace=True)\n",
    "\n",
    "# # add 'complexity' column\n",
    "# df1['complexity'] = df1['text'].apply(lambda x: calculate_complexity(x, words))\n",
    "\n",
    "# # add generated column (make them all equal to 0)\n",
    "# # df1['generated'] = 0\n",
    "\n",
    "# df1 = df1[['text', 'generated', 'complexity']]\n",
    "\n",
    "# # Sort the combined dataset by 'complexity' in ascending order\n",
    "# df1 = df1.sort_values(by='complexity', ascending=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "df1.to_csv('Essays/ai_generated_essays_llm_detect_kaggle.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved to test_essays.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "parquet_file_path = 'valid_essays.parquet'\n",
    "\n",
    "# Specify the path where you want to save the CSV file\n",
    "csv_file_path = 'test_essays.csv'\n",
    "\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f'CSV file has been saved to {csv_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before human 23060\n",
      "Before ai 10265\n",
      "After:  23060\n",
      "After:  26011\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import complex\n",
    "\n",
    "# Define the provided functions here\n",
    "\n",
    "def clean_word(word):\n",
    "    word = str(word)\n",
    "    word = word.lower()  # Lowercase the word\n",
    "    # Remove leading/trailing/inter-word punctuation\n",
    "    cleaned_word = re.sub(r'(?<!\\w)[\\'\\\"?!;:,.]|(?<=\\s)[\\'\\\"?!;:,.]|[\\'\\\"?!;:,.](?!\\w)', '', word)\n",
    "    return cleaned_word\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    cleaned_words = [clean_word(word) for word in words if clean_word(word)]\n",
    "    return cleaned_words\n",
    "\n",
    "def calculate_complexity(text: str, dic: dict) -> float:\n",
    "    text = str(text)\n",
    "    words = clean_text(text)\n",
    "    complexity = 0.0\n",
    "    values = []\n",
    "    for word in words:\n",
    "        if dic.get(word) is not None:\n",
    "            complexity += dic[word]\n",
    "            values.append(dic[word])\n",
    "    if values:\n",
    "        return np.median(values)\n",
    "    else:\n",
    "        return complexity  # Return 0 or a default value if no words match\n",
    "\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "df1 = pd.read_csv('megaset.csv')\n",
    "\n",
    "# print count of human essays before merging\n",
    "\n",
    "# print non numeric values in 'generated' column\n",
    "\n",
    "df2 = pd.read_csv('Essays/ai_generated_essays_llm_detect_kaggle.csv')\n",
    "\n",
    "\n",
    "print(\"Before human\", len(df1[df1['generated'] == 0]))\n",
    "print(\"Before ai\", len(df1[df1['generated'] == 1]))\n",
    "\n",
    "percentage = 0.5  # For example, to drop 25% of the rows where 'generated' == 1\n",
    "\n",
    "# # Determine how many rows to drop\n",
    "# rows_to_drop = int(len(df1[df1['generated'] == 0]) * percentage)\n",
    "\n",
    "# # Randomly choose indices of rows to drop\n",
    "# indices_to_drop = np.random.choice(df1[df1['generated'] == 0].index, rows_to_drop, replace=False)\n",
    "\n",
    "# # Drop those rows\n",
    "# df1 = df1.drop(indices_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "# Combine the datasets\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Calculate complexity for each essay\n",
    "combined_df['complexity'] = combined_df['text'].apply(lambda x: calculate_complexity(x, words))\n",
    "\n",
    "combined_df = combined_df[['text', 'generated', 'complexity', 'variance', 'length']]\n",
    "\n",
    "# Sort the combined dataset by 'complexity' in ascending order\n",
    "combined_df = combined_df.sort_values(by='complexity', ascending=True)\n",
    "\n",
    "#  print count of human essays after merging\n",
    "print(\"After: human \", len(combined_df[combined_df['generated'] == 0]))\n",
    "print(\"After: ai \", len(combined_df[combined_df['generated'] == 1]))\n",
    "\n",
    "\n",
    "\n",
    "# Save the combined dataset\n",
    "# combined_df.to_csv('merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 285383 entries, 0 to 285382\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       285383 non-null  object\n",
      " 1   generated  285383 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('persuade_corpus_1.0.csv')\n",
    "# df.rename(columns={'full_text': 'text'}, inplace=True)\n",
    "# df['generated'] = 0\n",
    "# df = df[['text', 'generated']]\n",
    "df.info()\n",
    "df.to_csv('persuade_corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of texts in set1 that are in set2: 19.73%\n",
      "Percentage of duplicate texts: 4.51%\n",
      "Percentage of duplicate texts: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_text_overlap_percentage(path_to_set1, path_to_set2):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of texts in set1 that are also in set2.\n",
    "\n",
    "    Parameters:\n",
    "    - path_to_set1: The file path to the first dataset.\n",
    "    - path_to_set2: The file path to the second dataset.\n",
    "\n",
    "    Returns:\n",
    "    - The percentage of texts in set1 that are also in set2.\n",
    "    \"\"\"\n",
    "    # Load the datasets\n",
    "    set1 = pd.read_csv(path_to_set1)\n",
    "    set2 = pd.read_csv(path_to_set2)\n",
    "    \n",
    "    # Extract the 'text' columns\n",
    "    texts_set1 = set(set1['text'])\n",
    "    texts_set2 = set(set2['text'])\n",
    "    \n",
    "    # Determine the number of texts in set1 that are also in set2\n",
    "    overlap_count = len(texts_set1.intersection(texts_set2))\n",
    "    \n",
    "    # Calculate the percentage\n",
    "    percentage_overlap = (overlap_count / len(texts_set1)) * 100\n",
    "    \n",
    "    return percentage_overlap\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def remove_common_texts(path_to_set1, path_to_set2, path_to_modified_set2=None):\n",
    "    \"\"\"\n",
    "    Removes texts from the second dataset that are present in both datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - path_to_set1: The file path to the first dataset.\n",
    "    - path_to_set2: The file path to the second dataset.\n",
    "    - path_to_modified_set2: Optional. The file path where the modified second dataset should be saved.\n",
    "\n",
    "    Returns:\n",
    "    - The modified second dataset as a DataFrame if path_to_modified_set2 is None.\n",
    "    \"\"\"\n",
    "    # Load the datasets\n",
    "    set1 = pd.read_csv(path_to_set1)\n",
    "    set2 = pd.read_csv(path_to_set2)\n",
    "    \n",
    "    # Identify texts that are common to both datasets\n",
    "    common_texts = set(set1['text']).intersection(set(set2['text']))\n",
    "    \n",
    "    # Remove these common texts from the second dataset\n",
    "    modified_set2 = set2[~set2['text'].isin(common_texts)]\n",
    "    \n",
    "    # Optionally, save the modified second dataset to a file\n",
    "    if path_to_modified_set2:\n",
    "        modified_set2.to_csv(path_to_modified_set2, index=False)\n",
    "    else:\n",
    "        return modified_set2\n",
    "    \n",
    "\n",
    "def add_unique_texts_from_set1_to_set2(path_to_set1, path_to_set2, path_to_modified_set2):\n",
    "    \"\"\"\n",
    "    Adds non-overlapping texts from the first dataset to the second dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - path_to_set1: The file path to the first dataset.\n",
    "    - path_to_set2: The file path to the second dataset.\n",
    "    - path_to_modified_set2: The file path where the modified second dataset should be saved.\n",
    "    \"\"\"\n",
    "    # Load the datasets\n",
    "    set1 = pd.read_csv(path_to_set1)\n",
    "    set2 = pd.read_csv(path_to_set2)\n",
    "    \n",
    "    # Identify texts that are unique to set1 (not present in set2)\n",
    "    unique_texts_to_set1 = set1[~set1['text'].isin(set2['text'])]\n",
    "    \n",
    "    # Combine set2 with unique texts from set1\n",
    "    modified_set2 = pd.concat([set2, unique_texts_to_set1], ignore_index=True)\n",
    "    \n",
    "    # Save the modified second dataset to the file\n",
    "    modified_set2.to_csv(path_to_modified_set2, index=False)\n",
    "\n",
    "    \n",
    "def calculate_duplicate_percentage(df, text_column='text'):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of duplicate texts within a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to analyze.\n",
    "    - text_column: The name of the column containing the texts. Defaults to 'text'.\n",
    "\n",
    "    Returns:\n",
    "    - The percentage of duplicate texts within the DataFrame.\n",
    "    \"\"\"\n",
    "    total_texts = len(df)\n",
    "    unique_texts = len(df[text_column].unique())\n",
    "    duplicate_texts = total_texts - unique_texts\n",
    "    percentage_duplicates = (duplicate_texts / total_texts) * 100\n",
    "    \n",
    "    return percentage_duplicates\n",
    "\n",
    "\n",
    "\n",
    "def remove_duplicates(df, text_column='text'):\n",
    "    \"\"\"\n",
    "    Removes all duplicate texts from a DataFrame, keeping only the first occurrence.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "    - text_column: The name of the column containing the texts. Defaults to 'text'.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    # Remove duplicates based on the text_column and keep the first occurrence\n",
    "    df_cleaned = df.drop_duplicates(subset=[text_column], keep='first')\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'dataset' is your DataFrame and it has a column named '\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Or, if you want to work with the modified DataFrame directly:\n",
    "# modified_set2 = remove_common_texts(path_to_set1, path_to_set2)\n",
    "# print(modified_set2)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# path_to_set2 = 'train_v2_drct_02.csv'\n",
    "path_to_set1 = \"train_essays.csv\"\n",
    "# path_to_set1 = 'train_v2_drct_02.csv'\n",
    "\n",
    "\n",
    "\n",
    "# path_to_set1 = 'metadataset.csv'\n",
    "# # path_to_set1 = \"megaset2.csv\"\n",
    "# df1 = pd.read_csv(path_to_set1)\n",
    "\n",
    "\n",
    "# # calculate ai count\n",
    "# print(\"Before ai\", len(df1[df1['generated'] == 1]))\n",
    "# print(\"Before human\", len(df1[df1['generated'] == 0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # path_to_set2 = \"human.csv\"\n",
    "path_to_set2 = 'megaset4.csv'\n",
    "# df2 = pd.read_csv(path_to_set2)\n",
    "\n",
    "# # # calculate ai count\n",
    "# print(\"Before ai\", len(df2[df2['generated'] == 1]))\n",
    "# print(\"Before human\", len(df2[df2['generated'] == 0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # This will save the modified second dataset to a new file\n",
    "# percentage = calculate_text_overlap_percentage(path_to_set1, path_to_set2)\n",
    "# print(f\"Percentage of texts in set1 that are in set2: {percentage:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "destination = 'megaset4.csv'\n",
    "# df = pd.read_csv(path_to_set2)\n",
    "\n",
    "# add_unique_texts_from_set1_to_set2(path_to_set1, path_to_set2, destination)\n",
    "\n",
    "\n",
    "\n",
    "# percentage_duplicates = calculate_duplicate_percentage(df, 'text')\n",
    "# print(f\"Percentage of duplicate texts: {percentage_duplicates:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "percentage = calculate_text_overlap_percentage(path_to_set1, destination)\n",
    "print(f\"Percentage of texts in set1 that are in set2: {percentage:.2f}%\")\n",
    "# df = pd.read_csv(destination)\n",
    "# df = remove_duplicates(df)\n",
    "# df.to_csv(path_to_set1, index=False)\n",
    "\n",
    "df = pd.read_csv(path_to_set1)\n",
    "print(f\"Percentage of duplicate texts: {calculate_duplicate_percentage(df, 'text'):.2f}%\")\n",
    "#df = remove_duplicates(df)\n",
    "\n",
    "df = remove_duplicates(df)\n",
    "print(f\"Percentage of duplicate texts: {calculate_duplicate_percentage(df, 'text'):.2f}%\")\n",
    "\n",
    "df.to_csv(path_to_set1, index=False)\n",
    "\n",
    "\n",
    "# path_to_set1 = 'train_v2_drct_02.csv'\n",
    "# path_to_set2 = 'final_test.csv'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
